{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96368aa1-9948-45eb-bef9-7e2a717359fb",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a5d73-2d30-4e70-bcbd-62f045886896",
   "metadata": {},
   "source": [
    "In this notebook, we convert the .csv files to .h5 files, and ensure that the idtracker.ai number of frames match the sLEAP number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0637c-9949-425d-9f4a-1d16af948afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92332f53-5148-4857-a432-a6126d78cac0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e38e77-5350-4d9f-982c-ffc067e9a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "import os\n",
    "%autoreload 2\n",
    "import sys\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca446c2-5630-4f99-83c5-51716122f2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaaf4ad-583c-4060-8da1-4702e9f957b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e597ce-4885-42ea-b6da-252ce11d89ca",
   "metadata": {},
   "source": [
    "# Prepare filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154ab55-4bac-4344-b0b3-1d4c05514bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8471359-c0ab-41ce-aa35-30ea3403c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_folder = '/media/liam/hd1/fighting_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f5b04-25bb-4144-a179-d9249b85c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b290ee-f3ab-4d98-968b-e8e4a5e7d6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/liam/hd1/fighting_data/FishTank20200127_143538',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200129_140656',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200130_153857',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200130_181614',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200207_161445',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200213_154940',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200214_153519',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200217_160052',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200218_153008',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200316_163320',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200327_154737',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200330_161100',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200331_162136',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200520_152810',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200521_154541',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200525_161602',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200526_160100',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200527_152401',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200824_151740',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200828_155504',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200902_160124',\n",
       " '/media/liam/hd1/fighting_data/FishTank20200903_160946']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather all the top level experiment filepaths\n",
    "exp_folder_paths = glob.glob(main_data_folder+'/*')\n",
    "exp_folder_paths.sort()\n",
    "exp_folder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d806816a-31d7-4ff7-89bb-9bd5a00b59e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_folder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81979a8f-6738-4000-b1d7-9f2c88e6601c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c40bd5d-214a-4073-98a0-8bd16c39796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FishTank20200127_143538',\n",
       " 'FishTank20200129_140656',\n",
       " 'FishTank20200130_153857',\n",
       " 'FishTank20200130_181614',\n",
       " 'FishTank20200207_161445',\n",
       " 'FishTank20200213_154940',\n",
       " 'FishTank20200214_153519',\n",
       " 'FishTank20200217_160052',\n",
       " 'FishTank20200218_153008',\n",
       " 'FishTank20200316_163320',\n",
       " 'FishTank20200327_154737',\n",
       " 'FishTank20200330_161100',\n",
       " 'FishTank20200331_162136',\n",
       " 'FishTank20200520_152810',\n",
       " 'FishTank20200521_154541',\n",
       " 'FishTank20200525_161602',\n",
       " 'FishTank20200526_160100',\n",
       " 'FishTank20200527_152401',\n",
       " 'FishTank20200824_151740',\n",
       " 'FishTank20200828_155504',\n",
       " 'FishTank20200902_160124',\n",
       " 'FishTank20200903_160946']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the exps names\n",
    "expNames = [os.path.basename(folderpath) for folderpath in exp_folder_paths]\n",
    "expNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3d6a1-6be1-4e81-995c-ec5c785994b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453f0f01-7d4a-465e-b50e-c9d458d9616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_names = ['D_xz', 'E_xy', 'F_yz']\n",
    "numCams = len(cam_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf65df-68b3-481e-9fe8-0d1d7441c575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f71ce5-1459-4264-bd12-2a2f4fb75a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "218895bb-6569-4b9f-9a68-b4ea3eec61c8",
   "metadata": {},
   "source": [
    "# Check that sLEAP and idtracker give us the same total number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b94ac-05ed-419d-af38-2faea2531629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc36c3be-5b82-41db-9452-0a6e873765ba",
   "metadata": {},
   "source": [
    "##  from the sleap csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c937df5-6a7d-4c43-92b4-c7129cb930fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "expIdx=0 has  different number of frames in last csv file. Using the max value for the calculation\n",
      "[5755, 5755, 5753]\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 971756,  769848,  501943,  517000,  463997,  730119,  495610,\n",
       "        601226,  537880, 1032353,  561010,  556428,  746434, 1248577,\n",
       "        596607,  588962,  689100,  497635, 1695643,  798382,  717814,\n",
       "        726000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expNumFrames_sLEAP = []\n",
    "\n",
    "for expIdx in range(len(expNames)):\n",
    "    print(expIdx)\n",
    "\n",
    "    # --- parse the folder paths for this experiment --- #\n",
    "    expFolderPath = exp_folder_paths[expIdx]\n",
    "    exp_cam_folders = [os.path.join(expFolderPath, 'sleap_results_csv', camName) for camName in cam_names]\n",
    "\n",
    "\n",
    "    # --- grab all the csv file paths ---- #\n",
    "    exp_cam_csv_paths = []\n",
    "    for camIdx in range(len(cam_names)):\n",
    "        cam_csv_paths = glob.glob(exp_cam_folders[camIdx]+'/*.csv')\n",
    "        cam_csv_paths.sort()\n",
    "        exp_cam_csv_paths.append(cam_csv_paths)\n",
    "\n",
    "\n",
    "    #  --- test that we have the same number of csv files in each camera view ---- #\n",
    "    num_files_for_each_cam = [len(csv_list) for csv_list in exp_cam_csv_paths]\n",
    "    if len(set(num_files_for_each_cam)) != 1:\n",
    "        raise TypeError(\"cam views have different number of csv files\")\n",
    "    # now set the total number of csv files, since all cams have the same number\n",
    "    num_csv_files = list(set(num_files_for_each_cam))[0]\n",
    "\n",
    "\n",
    "    #--- get the total number of frames in the experiment ---#\n",
    "    # Note on the method:\n",
    "    # all csv files, apart from the final one, will have 6000 frames.\n",
    "    # We count the number of frames in the last csv file.\n",
    "    # So numFrames = (numCsvFiles-1)*6000 + (numFrames_in_last_csv_file)\n",
    "\n",
    "    # count the number of frames from each camera view separately\n",
    "    last_mp4_final_frame_indices = []\n",
    "    for camIdx in range(len(cam_names)):\n",
    "        last_results_df = pd.read_csv(exp_cam_csv_paths[camIdx][-1], names=[\"frame_index\", \"instance_index\", \"point_index\", \"x\", \"y\"])\n",
    "        final_frame_index = last_results_df['frame_index'].values[-1]\n",
    "        last_mp4_final_frame_indices.append(final_frame_index)\n",
    "\n",
    "    # test that we have the same number\n",
    "    if len(set(last_mp4_final_frame_indices)) != 1:\n",
    "        print(f\"expIdx={expIdx} has  different number of frames in last csv file. Using the max value for the calculation\")\n",
    "        print(last_mp4_final_frame_indices)\n",
    "        print()\n",
    "        num_frames_in_last_mp4 = np.max(np.array(last_mp4_final_frame_indices)) + 1 # +1 to move from 0-indexing to cardinality\n",
    "        #raise TypeError(\"cam views have different number of frames in last csv file\")\n",
    "    else:\n",
    "        num_frames_in_last_mp4 = list(set(last_mp4_final_frame_indices))[0] + 1 # +1 to move from 0-indexing to cardinality\n",
    "\n",
    "    # now get the total number of frames\n",
    "    numFrames = (num_csv_files-1)*6000 + num_frames_in_last_mp4\n",
    "    \n",
    "    \n",
    "    # record\n",
    "    expNumFrames_sLEAP.append(numFrames)\n",
    "    \n",
    "expNumFrames_sLEAP = np.array(expNumFrames_sLEAP)\n",
    "expNumFrames_sLEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bdcab-db67-4621-b306-50729a0e3723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1226f-536a-4107-9129-93f22704d0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea6890e-8d71-447f-9a75-79c1c76f2717",
   "metadata": {},
   "source": [
    "## from idtracker results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75df9afe-632a-4370-ad62-7cdb8eea2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "expNumFrames_idtracker = []\n",
    "\n",
    "for expIdx in range(len(expNames)):\n",
    "    #print(expIdx)\n",
    "    \n",
    "    raw_idtracker_filepath = os.path.join(exp_folder_paths[expIdx], 'idtracker_results/trajectories.npy')\n",
    "    trajectories_dict = np.load(raw_idtracker_filepath, allow_pickle=True).item()\n",
    "    idtraj = trajectories_dict['trajectories']\n",
    "    numFrames = idtraj.shape[0]\n",
    "    expNumFrames_idtracker.append(numFrames)\n",
    "    \n",
    "expNumFrames_idtracker = np.array(expNumFrames_idtracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3fc611-92b2-49bb-9bbf-2e8487aa0321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 971756,  769848,  501943,  517000,  463997,  730119,  495610,\n",
       "        601226,  537880, 1032353,  561010,  556428,  746434, 1248577,\n",
       "        596607,  588962,  689100,  497635, 1695643,  798382,  717814,\n",
       "        726000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expNumFrames_idtracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01331977-1e43-4f45-bf5c-eb243c1f327f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23879e48-421b-4146-8bc9-1cd2c4c216a9",
   "metadata": {},
   "source": [
    "## compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e70936-6644-4010-9d26-8c1cb71daeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expNumFrames_sLEAP - expNumFrames_idtracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17cd4c-f294-4b61-989a-fa03153f04e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0447066-9e76-4550-b65b-62f52056c236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 971756,  769848,  501943,  517000,  463997,  730119,  495610,\n",
       "        601226,  537880, 1032353,  561010,  556428,  746434, 1248577,\n",
       "        596607,  588962,  689100,  497635, 1695643,  798382,  717814,\n",
       "        726000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now use either to set the total number of frames\n",
    "expNumFrames = expNumFrames_sLEAP\n",
    "expNumFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e532a3-9015-4d21-8814-e7286fc6f761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662fbd8-c98b-4211-8747-34956e32ccae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b98c74-2871-47e3-aaed-2ebec49231a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b45719b-1783-43f3-8f5f-f0d554dd0ab7",
   "metadata": {},
   "source": [
    "# convert csv format to multidimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399eaa4e-bb77-4895-b115-7a4b2fc206b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 971756,  769848,  501943,  517000,  463997,  730119,  495610,\n",
       "        601226,  537880, 1032353,  561010,  556428,  746434, 1248577,\n",
       "        596607,  588962,  689100,  497635, 1695643,  798382,  717814,\n",
       "        726000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expNumFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a6daad-0e49-49d9-bea7-774203ac1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFish = 2\n",
    "numBodyPoints = 3\n",
    "\n",
    "idtracker_acceleration_thresh = 3.3\n",
    "idtracker_nan_win = 6\n",
    "\n",
    "numProcessors = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36784cab-e70a-45b1-bd06-7cf684321722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39993814-43b1-4eb9-b237-854273f47957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_idtracker_data(idTracker_filepath, acceleration_thresh, NaN_window):\n",
    "    ''' A function to load idTracker results and parse them, any potential swaps\n",
    "        after collision events by results with very high accelerations\n",
    "        \n",
    "    --- args ---\n",
    "    idTracker_filepath:  the filepath to the trajectories.npy file for an experiment\n",
    "    acceleration_thresh: the threshold on the absolute value of the acceleration\n",
    "    NaN_window:          the number of frames either side of problematic frames to remove\n",
    "    \n",
    "    --- returns ---\n",
    "    idtraj: the processed idtracker timeseries.\n",
    "\n",
    "    -- Thanks to --\n",
    "    Simon Goorney\n",
    "    '''\n",
    "    # load the centroid timeseries\n",
    "    trajectories_dict = np.load(idTracker_filepath, allow_pickle=True).item()\n",
    "    idtraj = trajectories_dict['trajectories']\n",
    "\n",
    "    # compute the relevant derivatives\n",
    "    speed = np.gradient(idtraj, axis=0)\n",
    "    normspeed = np.linalg.norm(speed, axis=2)\n",
    "    normaccel = np.gradient(normspeed, axis=0)\n",
    "    absaccel = np.abs(normaccel)\n",
    "    \n",
    "    # compute the number of frames in the experiment\n",
    "    nfs = speed.shape[0]\n",
    "\n",
    "    def compare_nan_array(func, a, thresh):\n",
    "        # Thanks: https://stackoverflow.com/a/47340067\n",
    "        out = ~np.isnan(a)\n",
    "        out[out] = func(a[out] , thresh)\n",
    "        return out\n",
    "\n",
    "    # find problematic frames\n",
    "    super_threshold_indices = np.where(compare_nan_array(np.greater, absaccel, acceleration_thresh))\n",
    "    indices = super_threshold_indices[0]\n",
    "\n",
    "    # loop through problem frames, Nan'ing either side in a window\n",
    "    for i in indices:\n",
    "        if i < NaN_window:\n",
    "            idtraj[:i+int(NaN_window/2)] = np.NaN\n",
    "        elif nfs - i < NaN_window:\n",
    "            idtraj[i-int(NaN_window/2):] = np.NaN\n",
    "        else:\n",
    "            idtraj[i-int(NaN_window/2):i+int(NaN_window/2)] = np.NaN\n",
    "\n",
    "    # finish up\n",
    "    return idtraj\n",
    "\n",
    "\n",
    "\n",
    "def convert_csv_triple_to_array_PARALLEL(i):\n",
    "    ''' Given an index, which we use to parse a triplet of filepaths for csv file sleap results\n",
    "        for the three camera views of the same points in time,\n",
    "        return an array of the sleap results in the shape (numCams, numFrames, numFish, numBodypoints, 2).\n",
    "    '''\n",
    "    cam_names = ['D_xz', 'E_xy', 'F_yz']\n",
    "    numCams = len(cam_names)\n",
    "    \n",
    "    # parse the filepaths for this jobIdx \n",
    "    exp_csv_cam_paths = var_dict['experiment_csv_cam_paths']\n",
    "    csv_cam_paths = exp_csv_cam_paths[i]\n",
    "    #print(csv_cam_paths)\n",
    "    \n",
    "    # --- get the number of movie frames for this csv file ---- #\n",
    "    # Note: we need to be careful in case the csv files for the differnet camera views have differing values\n",
    "    #       So compute using all three views, and use the max\n",
    "    csv_final_frame_indices = []\n",
    "    for camIdx in range(numCams):\n",
    "        df = pd.read_csv(csv_cam_paths[camIdx], names=[\"frame_index\", \"instance_index\", \"point_index\", \"x\", \"y\"])\n",
    "        final_frame_index = df['frame_index'].values[-1]\n",
    "        csv_final_frame_indices.append(final_frame_index)\n",
    "    csv_numframes = np.max(np.array(csv_final_frame_indices)) + 1 # +1 to move from 0-indexing to cardinality\n",
    "    \n",
    "    # --- preallocate the output for this csv file --- #\n",
    "    csv_sleap_data_array = np.zeros((numCams, csv_numframes, numFish, numBodyPoints, 2))*np.NaN\n",
    "\n",
    "    # --- enter the data, one camera at a time ---- #\n",
    "    for camIdx in range(len(cam_names)):\n",
    "\n",
    "        cam_results_df = pd.read_csv(csv_cam_paths[camIdx], names=[\"frame_index\", \"instance_index\", \"point_index\", \"x\", \"y\"])\n",
    "\n",
    "        for fIdx in range(csv_numframes):\n",
    "            # grab the frame data\n",
    "            frame_data = cam_results_df[cam_results_df['frame_index']==fIdx]\n",
    "            # get number of matched instances\n",
    "            tracked_instance_idxs = np.unique(frame_data['instance_index'].values)\n",
    "            num_frame_instances = len(tracked_instance_idxs)\n",
    "\n",
    "            # make a dictionary for mapping tracked_instance_idxs to fishIdxs.\n",
    "            # Note: tracked_instance_idxs runs through the whole csv file,\n",
    "            #       and was generated by some sLEAP temporal tracking.\n",
    "            #       But we don't want this sLEAP tracking. \n",
    "            #       Each frame, we only want to be able to tell apart the different detected animals.\n",
    "            #       So we map the tracked_instance_idxs to a zero indexed list each frame.\n",
    "            #       We remove the temporal tracking component, and reset each frame\n",
    "            #       e.g.:  tracked_instance_idxs = [34,35] \n",
    "            #              fishIdxs             -> [0, 1],\n",
    "            fish_index_dict = dict(zip(tracked_instance_idxs, [i for i in range(num_frame_instances)]))\n",
    "\n",
    "            # place each datapoint in the correct part of the array\n",
    "            for index, row in frame_data.iterrows():\n",
    "                csv_sleap_data_array[camIdx, \n",
    "                                     fIdx, \n",
    "                                     fish_index_dict[row['instance_index']], \n",
    "                                     int(row['point_index']), \n",
    "                                     :] = row[['x', 'y']].values\n",
    "    # ---- finish up -------- #\n",
    "    return csv_sleap_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "537457bb-7700-40b5-874c-ff989bb5d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "for expIdx in range(1, len(expNames)):\n",
    "    print(expIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d5bc5-6a18-4ab4-8e77-641d7fd124e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22463760-f225-4e33-933e-dd70ab402327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  finished: 289.67706990242004s\n",
      "\n",
      "2  finished: 503.78289675712585s\n",
      "\n",
      "3  finished: 721.5171310901642s\n",
      "\n",
      "4  finished: 886.4639909267426s\n",
      "\n",
      "5  finished: 1148.6182172298431s\n",
      "\n",
      "6  finished: 1356.9830145835876s\n",
      "\n",
      "7  finished: 1586.21426820755s\n",
      "\n",
      "8  finished: 1799.9946608543396s\n",
      "\n",
      "9  finished: 2233.0473012924194s\n",
      "\n",
      "10  finished: 2458.712166786194s\n",
      "\n",
      "11  finished: 2678.4258670806885s\n",
      "\n",
      "12  finished: 2968.509666442871s\n",
      "\n",
      "13  finished: 3423.8771572113037s\n",
      "\n",
      "14  finished: 3655.2619819641113s\n",
      "\n",
      "15  finished: 3877.9798069000244s\n",
      "\n",
      "16  finished: 4122.609823703766s\n",
      "\n",
      "17  finished: 4330.636656522751s\n",
      "\n",
      "18  finished: 4950.6605887413025s\n",
      "\n",
      "19  finished: 5254.85621213913s\n",
      "\n",
      "20  finished: 5515.95645403862s\n",
      "\n",
      "21  finished: 5782.770781993866s\n",
      "\n",
      "\n",
      "\n",
      "--- all finished ---\n",
      "5782.771513700485\n"
     ]
    }
   ],
   "source": [
    "# ----- Prepare the data for tracking ----- #\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "#for expIdx in range(len(expNames)):\n",
    "#for expIdx in [0]:\n",
    "for expIdx in range(1, len(expNames)):\n",
    "\n",
    "    # make the savepath for the gathered data\n",
    "    exp_savepath = os.path.join(exp_folder_paths[expIdx], expNames[expIdx]+'_sLEAP_and_idtracks.h5')\n",
    "\n",
    "\n",
    "    # --- parse the folder paths for this experiment --- #\n",
    "    expFolderPath = exp_folder_paths[expIdx]\n",
    "    exp_cam_folders = [os.path.join(expFolderPath, 'sleap_results_csv', camName) for camName in cam_names]\n",
    "\n",
    "    # --- grab all the csv file paths ---- #\n",
    "    exp_cam_csv_paths = []\n",
    "    for camIdx in range(len(cam_names)):\n",
    "        cam_csv_paths = glob.glob(exp_cam_folders[camIdx]+'/*.csv')\n",
    "        cam_csv_paths.sort()\n",
    "        exp_cam_csv_paths.append(cam_csv_paths)\n",
    "\n",
    "    #  --- test that we have the same number of csv files in each camera view ---- #\n",
    "    num_files_for_each_cam = [len(csv_list) for csv_list in exp_cam_csv_paths]\n",
    "    if len(set(num_files_for_each_cam)) != 1:\n",
    "        raise TypeError(\"cam views have different number of csv files\")\n",
    "    # now set the total number of csv files, since all cams have the same number\n",
    "    num_csv_files = list(set(num_files_for_each_cam))[0]\n",
    "\n",
    "\n",
    "    # ---- create a list of filepaths for parallelization ---- #\n",
    "    exp_csv_cam_paths = [ [exp_cam_csv_paths[camIdx][csv_idx] for camIdx in range(len(cam_names))] for csv_idx in range(num_csv_files) ]\n",
    "    job_idxs = [i for i in range(len(exp_csv_cam_paths))]\n",
    "    #job_idxs = job_idxs[:20]\n",
    "\n",
    "\n",
    "    # ---- create a dictionary for access to this function ----- #\n",
    "    var_dict = {}\n",
    "    def init_worker(experiment_csv_cam_paths):\n",
    "        var_dict['experiment_csv_cam_paths'] = experiment_csv_cam_paths\n",
    "\n",
    "\n",
    "    # ---- map our function over all jobs ------ #\n",
    "    with Pool(processes=numProcessors, initializer=init_worker, initargs=(exp_csv_cam_paths,)) as pool:\n",
    "        outputs = pool.map(convert_csv_triple_to_array_PARALLEL, job_idxs)\n",
    "\n",
    "\n",
    "    # ---- concatenate the results into one array for this experiment ---- #\n",
    "    sleap_data = np.concatenate(outputs, axis=1)\n",
    "\n",
    "\n",
    "    # ---- now load and process the idtracker data ----- #\n",
    "    raw_idtracker_filepath = os.path.join(exp_folder_paths[expIdx], 'idtracker_results/trajectories.npy')\n",
    "    trajectories_dict = np.load(raw_idtracker_filepath, allow_pickle=True).item()\n",
    "    idtraj = trajectories_dict['trajectories']\n",
    "    idtracker_data = load_and_process_idtracker_data(raw_idtracker_filepath, idtracker_acceleration_thresh, idtracker_nan_win)\n",
    "\n",
    "\n",
    "    # -------- save the ouputs -------#\n",
    "    with h5py.File(exp_savepath, 'w') as hf:\n",
    "        hf.create_dataset('idTracker_data', data=idtracker_data)\n",
    "        hf.create_dataset('sLEAP_data', data=sleap_data)\n",
    "\n",
    "\n",
    "    print(expIdx, ' finished: {0}s'.format(time.time()-t0))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('--- all finished ---')\n",
    "tE = time.time()\n",
    "print(tE-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b7f8c-39d7-4163-ac04-2396b49d1f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac1731b5-fae0-4b3e-9753-7296d3dfb7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.36666666666666"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5782/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b5cb6-689e-45a9-9354-42cf1402a9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a63d9-b556-4db7-9a66-929d464cca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db8263-16ff-4ef7-86b9-51b488e200c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
